import whisper
import torch
import re
import json
from pathlib import Path
from typing import List, Dict, Tuple
from collections import defaultdict
import numpy as np
import os
from abc import ABC, abstractmethod
from app.services.intonation_analyzer import analyze_intonation
from app.services.speed_analyzer import analyze_speed

# JAVA_HOME ì„¤ì • (KoNLPy ì‚¬ìš©ì„ ìœ„í•´)
os.environ['JAVA_HOME'] = r'C:\Program Files\Java\jdk-17'

class STTEngine(ABC):
    """STT ì—”ì§„ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def transcribe(self, file_path: str) -> Dict:
        pass

class WhisperSTT(STTEngine):
    """Whisper STT ì—”ì§„"""
    
    def __init__(self, model_size: str = "medium"):
        self.model_size = model_size
        self.model = None
        
    def load_model(self):
        if self.model is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ”¥ Using device: {device}")
            self.model = whisper.load_model(self.model_size, device=device)
        return self.model
    
    def transcribe(self, file_path: str) -> Dict:
        """ìŒì„± íŒŒì¼ì„ ì™„ì „íˆ ë³€í™˜ (ëˆ„ë½ ì—†ì´)"""
        if not Path(file_path).exists():
            raise FileNotFoundError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        model = self.load_model()
        
        print(f"ğŸµ ìŒì„± íŒŒì¼ ë¶„ì„ ì¤‘: {file_path}")
        print("â³ ì „ì²´ ë¶„ì„ì„ ìœ„í•´ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        
        # ë§ë²„ë¦‡ í¬í•¨ì„ ìœ„í•œ ìµœì í™”ëœ Whisper ì„¤ì •
        result = model.transcribe(
            file_path,
            language="ko",
            task="transcribe",
            verbose=True,
            temperature=0.0,
            beam_size=5,
            best_of=5,
            patience=1.0,
            length_penalty=1.0,
            no_speech_threshold=0.6,
            logprob_threshold=-1.0,
            condition_on_previous_text=False,
            suppress_tokens=[-1],
            word_timestamps=True,
            initial_prompt=(
                "ë‹¤ìŒì€ í•œêµ­ì–´ ë°œí‘œ ìŒì„±ì…ë‹ˆë‹¤. 'ìŒ', 'ì–´', 'ì•„', 'ê·¸ë‹ˆê¹Œ', 'ì•„ë§ˆ', 'ê·¸ë˜ì„œ' ë“±ì˜ "
                "ëª¨ë“  ë§ë²„ë¦‡ê³¼ ê°íƒ„ì‚¬ë¥¼ í¬í•¨í•˜ì—¬ ì •í™•íˆ ì „ì‚¬í•´ì£¼ì„¸ìš”. ìƒëµí•˜ì§€ ë§ˆì„¸ìš”."
            )
        )
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ! {len(result.get('segments', []))}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ë°œê²¬")
        return result

class VoskSTT(STTEngine):
    """Vosk STT ì—”ì§„ (ë§ë²„ë¦‡ í¬í•¨ ê°€ëŠ¥)"""
    
    def __init__(self, model_path: str = None):
        try:
            import vosk
            self.vosk = vosk
            if model_path:
                self.model = vosk.Model(model_path)
            else:
                # ê¸°ë³¸ í•œêµ­ì–´ ëª¨ë¸ ê²½ë¡œ (ë‹¤ìš´ë¡œë“œ í•„ìš”)
                self.model = vosk.Model("models/vosk-model-ko-0.22")
            self.recognizer = vosk.KaldiRecognizer(self.model, 16000)
        except ImportError:
            raise ImportError("Voskê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install voskë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    
    def transcribe(self, file_path: str) -> Dict:
        import wave
        import json
        
        segments = []
        
        # WAV íŒŒì¼ì„ voskë¡œ ì²˜ë¦¬
        wf = wave.open(file_path, 'rb')
        
        segment_id = 0
        current_text = ""
        start_time = 0
        
        while True:
            data = wf.readframes(4000)
            if len(data) == 0:
                break
                
            if self.recognizer.AcceptWaveform(data):
                result = json.loads(self.recognizer.Result())
                if result.get('text'):
                    segments.append({
                        'id': segment_id,
                        'start': start_time,
                        'end': start_time + (len(data) / 16000),
                        'text': result['text'],
                        'tokens': [],  # Vosk token ì •ë³´ ì¶”ê°€ ê°€ëŠ¥
                        'words': []
                    })
                    segment_id += 1
                    start_time += len(data) / 16000
        
        # ë§ˆì§€ë§‰ ê²°ê³¼ ì²˜ë¦¬
        final_result = json.loads(self.recognizer.FinalResult())
        if final_result.get('text'):
            segments.append({
                'id': segment_id,
                'start': start_time,
                'end': start_time + 0.5,
                'text': final_result['text'],
                'tokens': [],
                'words': []
            })
        
        wf.close()
        
        return {
            'segments': segments,
            'language': 'ko',
            'text': ' '.join([seg['text'] for seg in segments])
        }

class GoogleSTT(STTEngine):
    """Google Speech-to-Text API"""
    
    def __init__(self, credentials_path: str = None):
        try:
            from google.cloud import speech
            self.speech = speech
            if credentials_path:
                os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path
        except ImportError:
            raise ImportError("Google Cloud Speech APIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def transcribe(self, file_path: str) -> Dict:
        client = self.speech.SpeechClient()
        
        with open(file_path, 'rb') as audio_file:
            content = audio_file.read()
        
        audio = self.speech.RecognitionAudio(content=content)
        config = self.speech.RecognitionConfig(
            encoding=self.speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code="ko-KR",
            enable_word_time_offsets=True,
            enable_automatic_punctuation=False,  # ë§ë²„ë¦‡ ë³´ì¡´ìœ„í•´ êµ¬ë‘ì  ìë™ ì¶”ê°€ ë¹„í™œì„±í™”
            profanity_filter=False,  # ëª¨ë“  ë‹¨ì–´ í¬í•¨
            speech_contexts=[{
                'phrases': ['ìŒ', 'ì–´', 'ê·¸ë‹ˆê¹Œ', 'ì•„ë§ˆ', 'ê·¸ë˜ì„œ', 'ë­ëƒë©´'],
                'boost': 20.0  # ë§ë²„ë¦‡ ì¸ì‹ ê°•í™”
            }]
        )
        
        response = client.recognize(config=config, audio=audio)
        
        segments = []
        for i, result in enumerate(response.results):
            alternative = result.alternatives[0]
            
            # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
            words = []
            if hasattr(alternative, 'words'):
                for word_info in alternative.words:
                    words.append({
                        'word': word_info.word,
                        'start_time': word_info.start_time.total_seconds(),
                        'end_time': word_info.end_time.total_seconds()
                    })
            
            start_time = words[0]['start_time'] if words else 0
            end_time = words[-1]['end_time'] if words else 0
            
            segments.append({
                'id': i,
                'start': start_time,
                'end': end_time,
                'text': alternative.transcript,
                'confidence': alternative.confidence,
                'words': words,
                'tokens': []
            })
        
        return {
            'segments': segments,
            'language': 'ko',
            'text': ' '.join([seg['text'] for seg in segments])
        }

class FillerWordDetector:
    """ì™„ì „í•œ ë§ë²„ë¦‡ íƒì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, stt_engine: str = "whisper", model_size: str = "medium"):
        self.stt_engine_name = stt_engine
        
        # STT ì—”ì§„ ì´ˆê¸°í™”
        if stt_engine == "whisper":
            self.stt_engine = WhisperSTT(model_size)
        elif stt_engine == "vosk":
            self.stt_engine = VoskSTT()
        elif stt_engine == "google":
            self.stt_engine = GoogleSTT()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” STT ì—”ì§„: {stt_engine}")
        
        # ë§ë²„ë¦‡ ëª©ë¡
        self.filler_words = [
            "ìŒ", "ì–´", "ê·¸ë‹ˆê¹Œ", "ì•„ë§ˆ", "ê·¸ë˜ì„œ", "ë­ëƒë©´", 
            "ì €ê¸°", "ê·¸ê²Œ", "ê·¸ëŸ¬ë‹ˆê¹Œ", "ì•„", "ë„¤", "ì˜ˆ"
        ]
        
        # ë§ë²„ë¦‡ íŒ¨í„´ (ì •ê·œì‹)
        self.filler_patterns = [
            r'\b(ìŒ|ì–´|ê·¸ë‹ˆê¹Œ|ì•„ë§ˆ|ê·¸ë˜ì„œ|ë­ëƒë©´|ì €ê¸°|ê·¸ê²Œ|ê·¸ëŸ¬ë‹ˆê¹Œ)\b',
            r'(ìŒ{2,}|ì–´{2,})',
            r'(ìŒ[.\s]{1,3}|ì–´[.\s]{1,3})',
            r'(ê·¸[ìœ¼ìŒ]*ë‹ˆê¹Œ|ê·¸[ìœ¼ìŒ]*ë˜ì„œ)',
            r'(ìŒ\s*ìŒ|ì–´\s*ì–´|ê·¸ë‹ˆê¹Œ\s*ê·¸ë‹ˆê¹Œ)',
            r'ê·¸[.\s]*ë‹ˆê¹Œ',
            r'ì•„[.\s]*ë§ˆ',
            r'ê·¸[.\s]*ë˜ì„œ',
            r'ë­[.\s]*ëƒë©´'
        ]
        
        self.confidence_threshold = 0.3
    
    def detect_fillers_in_text(self, text: str) -> Dict[str, int]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë§ë²„ë¦‡ íƒì§€"""
        filler_counts = defaultdict(int)
        text_lower = text.lower()
        
        # ê¸°ë³¸ ë‹¨ì–´ ë§¤ì¹­
        for filler in self.filler_words:
            pattern = rf'\b{re.escape(filler)}\b'
            matches = re.findall(pattern, text_lower)
            filler_counts[filler] += len(matches)
        
        # íŒ¨í„´ ë§¤ì¹­
        for pattern in self.filler_patterns:
            matches = re.finditer(pattern, text_lower)
            for match in matches:
                matched_text = match.group()
                if any(filler in matched_text for filler in ["ìŒ", "ã…¡", "ìœ¼"]):
                    filler_counts["ìŒ"] += 1
                elif any(filler in matched_text for filler in ["ì–´", "ã…“"]):
                    filler_counts["ì–´"] += 1
                elif "ë‹ˆê¹Œ" in matched_text:
                    filler_counts["ê·¸ë‹ˆê¹Œ"] += 1
                elif "ë˜ì„œ" in matched_text:
                    filler_counts["ê·¸ë˜ì„œ"] += 1
                elif "ë§ˆ" in matched_text and "ì•„" in matched_text:
                    filler_counts["ì•„ë§ˆ"] += 1
                elif "ëƒë©´" in matched_text:
                    filler_counts["ë­ëƒë©´"] += 1
        
        return dict(filler_counts)
    
    def analyze_segments(self, segments: List[Dict]) -> List[Dict]:
        """ì„¸ê·¸ë¨¼íŠ¸ë³„ ë§ë²„ë¦‡ ë¶„ì„"""
        analyzed_segments = []
        
        for i, segment in enumerate(segments):
            text = segment.get("text", "").strip()
            start_time = segment.get("start", 0)
            end_time = segment.get("end", 0)
            
            # ë§ë²„ë¦‡ íƒì§€
            filler_counts = self.detect_fillers_in_text(text)
            total_fillers = sum(filler_counts.values())
            
            # í† í° ì •ë³´ ì²˜ë¦¬
            tokens = segment.get("tokens", [])
            words = segment.get("words", [])
            
            analyzed_segment = {
                "id": i + 1,
                "text": text,
                "start": round(start_time, 2),
                "end": round(end_time, 2),
                "duration": round(end_time - start_time, 2),
                "filler_counts": filler_counts,
                "total_fillers": total_fillers,
                "has_fillers": total_fillers > 0,
                "token_count": len(tokens),
                "word_count": len(words.split()) if isinstance(words, str) else len(words)
            }
            
            analyzed_segments.append(analyzed_segment)
        
        return analyzed_segments
    
    def detect_from_audio(self, file_path: str) -> Dict:
        """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
        print(f"ğŸ”Š STT ì—”ì§„ ì‚¬ìš©: {self.stt_engine_name}")
        
        # STT ì—”ì§„ìœ¼ë¡œ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        stt_result = self.stt_engine.transcribe(file_path)
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„
        segments = stt_result.get("segments", [])
        analyzed_segments = self.analyze_segments(segments)
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        total_stats = self.calculate_total_stats(analyzed_segments)
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "file_path": file_path,
            "stt_engine": self.stt_engine_name,
            "total_segments": len(analyzed_segments),
            "total_statistics": total_stats,
            "segments": analyzed_segments,
            "original_stt_result": stt_result
        }
        
        # ì–µì–‘ ë¶„ì„
        intonation_results, avg_pitch_std, pitch_ranges = analyze_intonation(file_path, segments)
        # ì†ë„ ë¶„ì„
        speed_results, avg_spm, avg_wpm = analyze_speed(file_path, segments)
        
        return result
    
    def calculate_total_stats(self, segments: List[Dict]) -> Dict:
        """ì „ì²´ í†µê³„ ê³„ì‚°"""
        total_filler_counts = defaultdict(int)
        segments_with_fillers = 0
        total_duration = 0
        
        for segment in segments:
            if segment["has_fillers"]:
                segments_with_fillers += 1
            
            for filler, count in segment["filler_counts"].items():
                total_filler_counts[filler] += count
            
            total_duration += segment["duration"]
        
        return {
            "total_fillers": sum(total_filler_counts.values()),
            "filler_breakdown": dict(total_filler_counts),
            "segments_with_fillers": segments_with_fillers,
            "filler_density": round(segments_with_fillers / len(segments) * 100, 2) if segments else 0,
            "total_duration": round(total_duration, 2)
        }
    
    def print_results(self, result: Dict):
        """ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ¯ ë§ë²„ë¦‡ íƒì§€ ê²°ê³¼")
        print("="*70)
        
        # ì „ì²´ í†µê³„
        stats = result["total_statistics"]
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  - ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {result['total_segments']}ê°œ")
        print(f"  - ë§ë²„ë¦‡ì´ ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸: {stats['segments_with_fillers']}ê°œ")
        print(f"  - ë§ë²„ë¦‡ ë¹ˆë„: {stats['filler_density']}%")
        print(f"  - ì´ ë¶„ì„ ì‹œê°„: {stats['total_duration']}ì´ˆ")
        print(f"  - ì´ ë§ë²„ë¦‡ ê°œìˆ˜: {stats['total_fillers']}ê°œ")
        
        # ë§ë²„ë¦‡ë³„ í†µê³„
        if stats['filler_breakdown']:
            print(f"\nğŸ—£ï¸ ë§ë²„ë¦‡ë³„ í†µê³„:")
            for filler, count in sorted(stats['filler_breakdown'].items(), 
                                       key=lambda x: x[1], reverse=True):
                percentage = round(count / stats['total_fillers'] * 100, 1)
                print(f"  - {filler}: {count}íšŒ ({percentage}%)")
        
        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ê²°ê³¼
        print(f"\nğŸ“ ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¶„ì„:")
        print("-"*70)
        
        for segment in result["segments"]:
            print(f"\n[{segment['id']}] {segment['start']}s ~ {segment['end']}s")
            print(f"í…ìŠ¤íŠ¸: {segment['text']}")
            
            if segment["has_fillers"]:
                print("ë§ë²„ë¦‡:", end=" ")
                filler_strs = []
                for filler, count in segment["filler_counts"].items():
                    filler_strs.append(f"{filler}({count})")
                print(", ".join(filler_strs))
            else:
                print("ë§ë²„ë¦‡: ì—†ìŒ")
        
        print("\n" + "="*70)
        print("âœ… ë¶„ì„ ì™„ë£Œ!")
        print("="*70)
    
    def save_to_json(self, result: Dict, output_path: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    audio_file = "test_audio/b1055308.wav"
    
    try:
        print("ğŸ§ª Whisper STT ì—”ì§„ í…ŒìŠ¤íŠ¸")
        
        # ë§ë²„ë¦‡ íƒì§€ê¸° ì´ˆê¸°í™”
        detector = FillerWordDetector(stt_engine="whisper", model_size="medium")
        
        # ë¶„ì„ ì‹¤í–‰
        result = detector.detect_from_audio(audio_file)
        
        # ê²°ê³¼ ì¶œë ¥
        detector.print_results(result)
        
        # JSON ì €ì¥
        output_json = "filler_analysis_whisper.json"
        detector.save_to_json(result, output_json)
        
        # ì–µì–‘ ë¶„ì„
        intonation_results, avg_pitch_std, pitch_ranges = analyze_intonation(audio_file, result["segments"])
        # ì†ë„ ë¶„ì„
        speed_results, avg_spm, avg_wpm = analyze_speed(audio_file, result["segments"])
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()