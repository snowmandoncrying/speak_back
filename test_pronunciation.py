import librosa
import numpy as np
import parselmouth
from faster_whisper import WhisperModel
import time
import tempfile
import soundfile as sf
import os
import torch
import torchaudio
from transformers import Wav2Vec2Processor, Wav2Vec2Model
from scipy.spatial.distance import cosine
import warnings
import hashlib
import torch.nn.functional as F
import re
from pydub import AudioSegment
import edge_tts
import asyncio
import subprocess

warnings.filterwarnings('ignore')

class AdvancedSpeechAnalyzer:
    def __init__(self):
        print("ğŸ”„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        # Whisper ëª¨ë¸ ì´ˆê¸°í™” (medium ëª¨ë¸ ì‚¬ìš©)
        self.model = WhisperModel("medium", device="cpu", compute_type="int8")
        # Wav2Vec2 ëª¨ë¸ ì´ˆê¸°í™”
        self.wav2vec_processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base")
        self.wav2vec_model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")
        self.wav2vec_model.eval()
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")

    def validate_text(self, text: str) -> bool:
        """ê¸°ì¤€ í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì‚¬"""
        if not text or not text.strip():
            print("âŒ ê¸°ì¤€ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return False
        return True

    def validate_audio_file(self, file_path: str) -> bool:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            if not os.path.exists(file_path):
                return False
            
            y, sr = librosa.load(file_path, sr=None, duration=0.1)
            return len(y) > 0 and sr > 0
        
        except Exception:
            return False

    def split_into_sentences(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
        # 1. í…ìŠ¤íŠ¸ ì •ê·œí™”
        text = text.strip()
        if not text:
            return []
        
        # 2. ë¬¸ì¥ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ê¸°ì¤€)
        sentences = re.split(r'[.!?]+', text)
        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ê³µë°± ì •ë¦¬
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences

    def convert_to_wav(self, input_path: str, output_path: str = None) -> str:
        """MP3/WAV íŒŒì¼ì„ ì˜¬ë°”ë¥¸ WAV í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if output_path is None:
                output_path = os.path.splitext(input_path)[0] + "_converted.wav"
            
            # ffmpegë¥¼ ì‚¬ìš©í•˜ì—¬ 16kHz, mono, PCM WAVë¡œ ë³€í™˜
            command = [
                "ffmpeg", "-y",
                "-i", input_path,
                "-ar", "16000",
                "-ac", "1",
                "-c:a", "pcm_s16le",
                output_path
            ]
            
            # ffmpeg ì‹¤í–‰ (ì¶œë ¥ ìˆ¨ê¹€)
            result = subprocess.run(command, capture_output=True, text=True)
            
            if result.returncode != 0:
                print(f"âŒ íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨: {result.stderr}")
                return None
            
            print(f"ğŸ”„ íŒŒì¼ ë³€í™˜ ì™„ë£Œ: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    async def create_reference_audio(self, text: str, output_path: str = "reference.wav") -> tuple:
        """ì „ì²´ ê¸°ì¤€ ìŒì„± ìƒì„± (edge-tts ì‚¬ìš©)"""
        try:
            print("\nğŸ¯ ê¸°ì¤€ ìŒì„± ìƒì„± ì¤‘...")
            
            # 1. í…ìŠ¤íŠ¸ ë¬¸ì¥ ë¶„ë¦¬
            sentences = self.split_into_sentences(text)
            if not sentences:
                print("âŒ ìœ íš¨í•œ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None, 0.0
            
            print(f"âœ… ì´ {len(sentences)}ê°œ ë¬¸ì¥ ì²˜ë¦¬ ì˜ˆì •")
            
            # 2. ë¬¸ì¥ë³„ ìŒì„± ìƒì„± ë° ê²°í•©
            combined_audio = None
            temp_files = []
            
            for i, sentence in enumerate(sentences, 1):
                print(f"\nğŸ”„ ë¬¸ì¥ {i}/{len(sentences)} ì²˜ë¦¬ ì¤‘...")
                print(f"ğŸ“ í…ìŠ¤íŠ¸: {sentence}")
                
                # 2.1 ì„ì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„± (MP3)
                temp_mp3 = os.path.join(tempfile.gettempdir(), f"temp_{i}.mp3")
                temp_wav = os.path.join(tempfile.gettempdir(), f"temp_{i}.wav")
                temp_files.extend([temp_mp3, temp_wav])
                
                try:
                    # 2.2 edge-ttsë¡œ ìŒì„± ìƒì„± (MP3)
                    communicate = edge_tts.Communicate(sentence, "ko-KR-SunHiNeural")
                    await communicate.save(temp_mp3)
                    print(f"âœ… TTS ìƒì„± (MP3): {temp_mp3}")
                    
                    # 2.3 MP3ë¥¼ WAVë¡œ ë³€í™˜
                    converted_wav = self.convert_to_wav(temp_mp3, temp_wav)
                    if not converted_wav:
                        continue
                    
                    # 2.4 ìŒì„± ê²°í•©
                    segment = AudioSegment.from_wav(converted_wav)
                    if combined_audio is None:
                        combined_audio = segment
                    else:
                        # ë¬¸ì¥ ì‚¬ì´ 0.5ì´ˆ ê°„ê²© ì¶”ê°€
                        combined_audio = combined_audio + AudioSegment.silent(duration=500) + segment
                        
                    print("âœ… ìŒì„± ìƒì„± ë° ë³€í™˜ ì„±ê³µ")
                    
                except Exception as e:
                    print(f"âŒ ë¬¸ì¥ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            # 3. ê²°ê³¼ ì €ì¥
            if combined_audio is None:
                print("âŒ ëª¨ë“  ë¬¸ì¥ ì²˜ë¦¬ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return None, 0.0
            
            try:
                # 3.1 ê²°í•©ëœ ìŒì„±ì„ MP3ë¡œ ì €ì¥
                temp_combined_mp3 = os.path.join(tempfile.gettempdir(), "combined.mp3")
                combined_audio.export(temp_combined_mp3, format="mp3")
                
                # 3.2 ìµœì¢… WAV ë³€í™˜
                final_wav = self.convert_to_wav(temp_combined_mp3, output_path)
                if not final_wav:
                    return None, 0.0
                
                # 3.3 ê¸¸ì´ ê³„ì‚°
                duration = len(combined_audio) / 1000.0  # ms â†’ ì´ˆ ë³€í™˜
                
                print(f"\nâœ… ê¸°ì¤€ ìŒì„± ìƒì„± ì™„ë£Œ: {output_path}")
                print(f"â±ï¸ ì´ ê¸¸ì´: {duration:.1f}ì´ˆ")
                print(f"ğŸ§ ê¸°ì¤€ ë°œìŒì„ ë“¤ìœ¼ë ¤ë©´ í•´ë‹¹ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
                
                return output_path, duration
                
            except Exception as e:
                print(f"âŒ ìµœì¢… íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                return None, 0.0
                
            finally:
                # 4. ì„ì‹œ íŒŒì¼ ì •ë¦¬
                for temp_file in temp_files + [temp_combined_mp3]:
                    try:
                        if os.path.exists(temp_file):
                            os.remove(temp_file)
                    except:
                        pass
                    
        except Exception as e:
            print(f"âŒ ê¸°ì¤€ ìŒì„± ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None, 0.0

    def load_audio_for_wav2vec(self, audio_path: str, start_time: float = None, end_time: float = None) -> torch.Tensor:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•˜ê³  Wav2Vec2 ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(audio_path):
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {audio_path}")
                return None
                
            # 2. íŒŒì¼ í˜•ì‹ í™•ì¸
            file_ext = os.path.splitext(audio_path)[1].lower()
            supported_formats = ['.wav', '.mp3', '.m4a', '.flac']
            if file_ext not in supported_formats:
                print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")
                return None
                
            # 3. ì˜¤ë””ì˜¤ ë¡œë“œ
            try:
                y, sr = librosa.load(audio_path, sr=16000)
            except Exception as e:
                print(f"âŒ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None
                
            # 4. êµ¬ê°„ ì¶”ì¶œ (ì§€ì •ëœ ê²½ìš°)
            if start_time is not None and end_time is not None:
                if start_time >= len(y)/sr or end_time > len(y)/sr:
                    print(f"âŒ ì˜ëª»ëœ êµ¬ê°„ ë²”ìœ„: {start_time}~{end_time}ì´ˆ (ì „ì²´ ê¸¸ì´: {len(y)/sr:.1f}ì´ˆ)")
                    return None
                    
                start_idx = int(start_time * sr)
                end_idx = int(end_time * sr)
                y = y[start_idx:end_idx]
                
                if len(y) == 0:
                    print("âŒ ì¶”ì¶œëœ êµ¬ê°„ì´ ë¹„ì–´ìˆìŒ")
                    return None
                
            # 5. ì •ê·œí™” ë° í…ì„œ ë³€í™˜
            if len(y) == 0:
                print("âŒ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ")
                return None
            
            y = librosa.util.normalize(y)
            audio_tensor = torch.FloatTensor(y)
            
            # 6. ì±„ë„ ì°¨ì› ì¶”ê°€
            if audio_tensor.ndim == 1:
                audio_tensor = audio_tensor.unsqueeze(0)
            
            return audio_tensor
            
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def extract_embeddings(self, waveform: torch.Tensor) -> torch.Tensor:
        """ìŒì„± ì„ë² ë”© ì¶”ì¶œ (Wav2Vec2 ì‚¬ìš©)"""
        try:
            if waveform is None or waveform.numel() == 0:
                print("âŒ ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ")
                return None
            
            # 1. ëª¨ë¸ ë¡œë“œ (ì‹±ê¸€í†¤ íŒ¨í„´)
            if not hasattr(self, 'wav2vec2_model'):
                try:
                    self.wav2vec2_model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")
                    self.wav2vec2_model.eval()
                    if torch.cuda.is_available():
                        self.wav2vec2_model = self.wav2vec2_model.cuda()
                except Exception as e:
                    print(f"âŒ Wav2Vec2 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    return None
                
            # 2. GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ë°ì´í„° ì´ë™
            if torch.cuda.is_available():
                waveform = waveform.cuda()
            
            # 3. ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬
            with torch.no_grad():
                try:
                    # ë°°ì¹˜ í¬ê¸° ê³„ì‚° (ë©”ëª¨ë¦¬ í•œê³„ ê³ ë ¤)
                    max_length = 30 * 16000  # 30ì´ˆ
                    if waveform.shape[1] > max_length:
                        segments = []
                        for i in range(0, waveform.shape[1], max_length):
                            segment = waveform[:, i:i+max_length]
                            features = self.wav2vec2_model(segment).last_hidden_state  # [1, seq_len, hidden_dim]
                            # ì‹œí€€ìŠ¤ ì°¨ì›ì— ëŒ€í•´ í‰ê·  í’€ë§
                            pooled = features.squeeze(0).mean(dim=0)  # [hidden_dim]
                            segments.append(pooled)
                        # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì˜ í‰ê· 
                        embeddings = torch.stack(segments).mean(dim=0)
                    else:
                        features = self.wav2vec2_model(waveform).last_hidden_state  # [1, seq_len, hidden_dim]
                        # ì‹œí€€ìŠ¤ ì°¨ì›ì— ëŒ€í•´ í‰ê·  í’€ë§
                        embeddings = features.squeeze(0).mean(dim=0)  # [hidden_dim]
                    
                    # 4. ì„ë² ë”© ê²€ì¦
                    if embeddings.dim() != 1:
                        print(f"âŒ ì˜ëª»ëœ ì„ë² ë”© ì°¨ì›: {embeddings.shape}")
                        return None
                        
                    if embeddings.shape[0] != 768:  # Wav2Vec2-baseì˜ hidden_dim
                        print(f"âŒ ì˜ëª»ëœ ì„ë² ë”© í¬ê¸°: {embeddings.shape}")
                        return None
                        
                    # 5. L2 ì •ê·œí™”
                    embeddings = F.normalize(embeddings, p=2, dim=0)
                    return embeddings.cpu()
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print("âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, CPUë¡œ ì „í™˜")
                        torch.cuda.empty_cache()
                        self.wav2vec2_model = self.wav2vec2_model.cpu()
                        waveform = waveform.cpu()
                        features = self.wav2vec2_model(waveform).last_hidden_state
                        embeddings = features.squeeze(0).mean(dim=0)
                        embeddings = F.normalize(embeddings, p=2, dim=0)
                        return embeddings
                    raise
                
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def calculate_similarity(self, user_emb: torch.Tensor, ref_emb: torch.Tensor) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ë° ì ìˆ˜í™”"""
        try:
            if user_emb is None or ref_emb is None:
                return 0
            
            # 1. ì„ë² ë”© ê²€ì¦
            if user_emb.dim() != 1 or ref_emb.dim() != 1:
                print(f"âš ï¸ ì˜ëª»ëœ ì„ë² ë”© ì°¨ì›: ì‚¬ìš©ì({user_emb.dim()}) vs ê¸°ì¤€({ref_emb.dim()})")
                return 0
            
            if user_emb.shape != ref_emb.shape:
                print(f"âš ï¸ ì„ë² ë”© í¬ê¸° ë¶ˆì¼ì¹˜: ì‚¬ìš©ì({user_emb.shape}) vs ê¸°ì¤€({ref_emb.shape})")
                return 0
            
            # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = F.cosine_similarity(user_emb.unsqueeze(0), ref_emb.unsqueeze(0), dim=1).item()
            
            # 3. ì ìˆ˜ ë³€í™˜ (0~100)
            score = max(0, min(100, (similarity + 1) * 50))  # [-1, 1] â†’ [0, 100]
            return score
            
        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0

    def get_similarity_feedback(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ í”¼ë“œë°± ìƒì„±"""
        if score >= 85:
            return "ë°œìŒì´ ë§¤ìš° ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤."
        elif score >= 65:
            return "ë°œìŒì´ ëŒ€ì²´ë¡œ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤."
        elif score >= 40:
            return "ì¼ë¶€ ë°œìŒì´ ë¶ˆëª…í™•í•˜ë‚˜ ì˜ë¯¸ ì „ë‹¬ì€ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        else:
            return "ë°œìŒì´ ë¶ˆëª…í™•í•˜ì—¬ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."

    def analyze_pronunciation(self, audio_path: str, start_time: float, end_time: float) -> dict:
        """ë°œìŒ í’ˆì§ˆ ë¶„ì„ (Parselmouth - fallbackìš©)"""
        try:
            # 1. ì˜¤ë””ì˜¤ ë¡œë“œ ë° êµ¬ê°„ ì¶”ì¶œ
            try:
                y, sr = librosa.load(audio_path)
                duration = len(y) / sr
            except Exception as e:
                return {
                    "quality": "ë¶„ì„ ë¶ˆê°€",
                    "feedback": f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}",
                    "score": 0.0
                }
            
            if start_time >= duration or end_time > duration:
                return {
                    "quality": "ë¶„ì„ ë¶ˆê°€",
                    "feedback": f"ìš”ì²­í•œ êµ¬ê°„ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤: {start_time:.1f}~{end_time:.1f}ì´ˆ (ì „ì²´ ê¸¸ì´: {duration:.1f}ì´ˆ)",
                    "score": 0.0
                }
            
            start_sample = max(0, int(start_time * sr))
            end_sample = min(len(y), int(end_time * sr))
            segment = y[start_sample:end_sample]
            
            if len(segment) == 0:
                return {
                    "quality": "ë¶„ì„ ë¶ˆê°€",
                    "feedback": "ì¶”ì¶œëœ êµ¬ê°„ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
                    "score": 0.0
                }
            
            # 2. ì„ì‹œ WAV íŒŒì¼ ì²˜ë¦¬
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_wav:
                try:
                    sf.write(temp_wav.name, segment, sr)
                    sound = parselmouth.Sound(temp_wav.name)
                except Exception as e:
                    return {
                        "quality": "ë¶„ì„ ë¶ˆê°€",
                        "feedback": f"ìŒì„± íŒŒì¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                        "score": 0.0
                    }
                    
                try:
                    # 3. í”¼ì¹˜ ë¶„ì„
                    pitch = sound.to_pitch()
                    pitch_values = pitch.selected_array['frequency']
                    valid_pitch = pitch_values[pitch_values > 0]
                    
                    if len(valid_pitch) == 0:
                        return {
                            "quality": "ë¶„ì„ ë¶ˆê°€",
                            "feedback": "ë°œìŒì„ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                            "score": 0.0
                        }
                    
                    # 4. ìŒì„± íŠ¹ì„± ë¶„ì„
                    pitch_std = np.std(valid_pitch)
                    
                    formant = sound.to_formant_burg()
                    f1_values = [formant.get_value_at_time(1, t) for t in formant.xs()]
                    f2_values = [formant.get_value_at_time(2, t) for t in formant.xs()]
                    
                    f1_std = np.std([f for f in f1_values if f != 0])
                    f2_std = np.std([f for f in f2_values if f != 0])
                    
                    # 5. ì ìˆ˜ ê³„ì‚°
                    stability_score = 100 - (pitch_std * 0.05 + f1_std * 0.025 + f2_std * 0.025)
                    stability_score = max(0, min(100, stability_score))
                    
                    # 6. í’ˆì§ˆ í‰ê°€
                    if stability_score >= 80:
                        quality = "ìš°ìˆ˜"
                        feedback = "ë°œìŒì´ ë§¤ìš° ëª…í™•í•˜ê³  ì•ˆì •ì ì…ë‹ˆë‹¤."
                    elif stability_score >= 65:
                        quality = "ì–‘í˜¸"
                        feedback = "ë°œìŒì´ ëŒ€ì²´ë¡œ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤."
                    elif stability_score >= 50:
                        quality = "ë³´í†µ"
                        feedback = "ë°œìŒì´ ì´í•´í•  ë§Œí•˜ë‚˜, ì¼ë¶€ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤."
                    else:
                        quality = "ë¯¸í¡"
                        feedback = "ë°œìŒì´ ë¶ˆì•ˆì •í•˜ì—¬ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
                    
                    return {
                        "quality": quality,
                        "feedback": feedback,
                        "score": stability_score
                    }
                    
                except Exception as e:
                    return {
                        "quality": "ë¶„ì„ ë¶ˆê°€",
                        "feedback": f"ìŒì„± íŠ¹ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                        "score": 0.0
                    }
                finally:
                    try:
                        os.remove(temp_wav.name)
                    except:
                        pass
                    
        except Exception as e:
            return {
                "quality": "ë¶„ì„ ì‹¤íŒ¨",
                "feedback": f"ë°œìŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "score": 0.0
            }

    def analyze_speech_rate(self, segment) -> tuple:
        """ë§ì†ë„(WPM) ë¶„ì„"""
        try:
            # ë‹¨ì–´ ìˆ˜ ê³„ì‚° (ê³µë°± ê¸°ì¤€)
            words = len(segment.text.split())
            duration = segment.end - segment.start
            
            if duration <= 0 or words == 0:
                return None, None
            
            # WPM (Words Per Minute) ê³„ì‚°
            wpm = (words / duration) * 60
            
            # ì†ë„ íŒì •
            if wpm < 90:
                feedback = "ë§ì†ë„ê°€ ë‹¤ì†Œ ëŠë¦½ë‹ˆë‹¤. ì¡°ê¸ˆ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì†ë„ë¡œ ë§í•´ë³´ì„¸ìš”."
                speed = "ëŠë¦¼"
            elif wpm <= 140:
                feedback = "ì ì ˆí•œ ë§ì†ë„ë¡œ ì²­ì¤‘ì´ ì´í•´í•˜ê¸° ì¢‹ìŠµë‹ˆë‹¤."
                speed = "ì ì ˆ"
            else:
                feedback = "ë§ì†ë„ê°€ ë‹¤ì†Œ ë¹ ë¦…ë‹ˆë‹¤. ì²­ì¤‘ì„ ìœ„í•´ ì¡°ê¸ˆ ë” ì²œì²œíˆ ë§í•´ë³´ì„¸ìš”."
                speed = "ë¹ ë¦„"
            
            return wpm, (speed, feedback)
            
        except Exception as e:
            print(f"âŒ ë§ì†ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None, None

    def detect_stuttering(self, text: str) -> tuple:
        """ë§ë”ë“¬ê¸° ê°ì§€"""
        try:
            import re
            
            # ì—°ì†ëœ ë‹¨ì–´/ìŒì ˆ ë°˜ë³µ íŒ¨í„´ ì°¾ê¸°
            # 1. "ì•„, ì•„, ì•„" í˜•íƒœ
            pattern1 = r'(\w+)[,\s]+\1[,\s]+\1+'
            # 2. "ì €ê¸°.. ì €ê¸°" í˜•íƒœ
            pattern2 = r'(\w+)[.]{2,}\s+\1+'
            
            matches = []
            for pattern in [pattern1, pattern2]:
                found = re.finditer(pattern, text)
                for match in found:
                    matches.append(match.group(1))
                
            if matches:
                repeated_words = ', '.join(set(matches))
                feedback = f"'{repeated_words}' ë¶€ë¶„ì—ì„œ ë§ë”ë“¬ì´ ê°ì§€ë©ë‹ˆë‹¤. ê¸´ì¥ì„ í’€ê³  ì²œì²œíˆ ë§í•´ë³´ì„¸ìš”."
                return repeated_words, feedback
            
            return None, None
            
        except Exception as e:
            print(f"âŒ ë§ë”ë“¬ê¸° ê°ì§€ ì˜¤ë¥˜: {e}")
            return None, None

    def analyze_silence(self, current_segment, previous_segment) -> tuple:
        """ë¬¸ì¥ ê°„ ì¹¨ë¬µ ê°ì§€"""
        try:
            if not previous_segment:
                return None, None
            
            silence_duration = current_segment.start - previous_segment.end
            
            # 0.5ì´ˆ ì´ìƒì˜ ì¹¨ë¬µì„ ê°ì§€
            if silence_duration >= 0.5:
                feedback = "ë¶ˆí•„ìš”í•œ ì¹¨ë¬µì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë” ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ ë§í•´ë³´ì„¸ìš”."
                return silence_duration, feedback
            
            return None, None
            
        except Exception as e:
            print(f"âŒ ì¹¨ë¬µ ê°ì§€ ì˜¤ë¥˜: {e}")
            return None, None

    def compare_segment_with_reference(self, user_audio: str, ref_audio: str, segment, ref_duration: float, previous_segment=None) -> dict:
        """ë¬¸ì¥ êµ¬ê°„ë³„ ë¹„êµ ë¶„ì„"""
        try:
            # 1. êµ¬ê°„ ì‹œê°„ ê³„ì‚° (padding 0.2ì´ˆ ì¶”ê°€)
            start_time = max(0, segment.start - 0.2)
            end_time = min(segment.end + 0.2, ref_duration)
            
            # êµ¬ê°„ ìœ íš¨ì„± ê²€ì‚¬
            if start_time >= end_time:
                return {
                    "text": segment.text,
                    "start": segment.start,
                    "end": segment.end,
                    "similarity_score": 0.0,
                    "similarity_feedback": "ë¹„êµ ë¶ˆê°€: ì˜ëª»ëœ êµ¬ê°„ ë²”ìœ„",
                    "pronunciation_quality": "ë¶„ì„ ë¶ˆê°€",
                    "pronunciation_score": 0.0,
                    "pronunciation_feedback": f"ì˜ëª»ëœ êµ¬ê°„ ë²”ìœ„: {start_time:.1f}~{end_time:.1f}ì´ˆ"
                }
            
            if end_time > ref_duration:
                return {
                    "text": segment.text,
                    "start": segment.start,
                    "end": segment.end,
                    "similarity_score": 0.0,
                    "similarity_feedback": "ë¹„êµ ë¶ˆê°€: ê¸°ì¤€ ìŒì„± ê¸¸ì´ ì´ˆê³¼",
                    "pronunciation_quality": "ë¶„ì„ ë¶ˆê°€",
                    "pronunciation_score": 0.0,
                    "pronunciation_feedback": f"ê¸°ì¤€ ìŒì„± ê¸¸ì´({ref_duration:.1f}ì´ˆ)ë¥¼ ì´ˆê³¼í•˜ëŠ” êµ¬ê°„ì…ë‹ˆë‹¤."
                }
            
            print(f"\nğŸŸ© ë¬¸ì¥: {segment.text}")
            print(f"â±ï¸ êµ¬ê°„: {start_time:.1f}ì´ˆ ~ {end_time:.1f}ì´ˆ")
            
            # 2. ì‚¬ìš©ì ìŒì„± êµ¬ê°„ ë¡œë“œ
            user_wave = self.load_audio_for_wav2vec(user_audio, start_time, end_time)
            if user_wave is None:
                return {
                    "text": segment.text,
                    "start": segment.start,
                    "end": segment.end,
                    "similarity_score": 0.0,
                    "similarity_feedback": "ë¹„êµ ë¶ˆê°€: ì‚¬ìš©ì ìŒì„± ë¡œë“œ ì‹¤íŒ¨",
                    "pronunciation_quality": "ë¶„ì„ ë¶ˆê°€",
                    "pronunciation_score": 0.0,
                    "pronunciation_feedback": "ì‚¬ìš©ì ìŒì„± êµ¬ê°„ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            print("âœ… ì‚¬ìš©ì êµ¬ê°„ ì¶”ì¶œ ì„±ê³µ")
            
            # 3. ê¸°ì¤€ ìŒì„± êµ¬ê°„ ë¡œë“œ
            ref_wave = self.load_audio_for_wav2vec(ref_audio, start_time, end_time)
            if ref_wave is None:
                return {
                    "text": segment.text,
                    "start": segment.start,
                    "end": segment.end,
                    "similarity_score": 0.0,
                    "similarity_feedback": "ë¹„êµ ë¶ˆê°€: ê¸°ì¤€ ìŒì„± ë¡œë“œ ì‹¤íŒ¨",
                    "pronunciation_quality": "ë¶„ì„ ë¶ˆê°€",
                    "pronunciation_score": 0.0,
                    "pronunciation_feedback": "ê¸°ì¤€ ìŒì„± êµ¬ê°„ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            print("âœ… AI êµ¬ê°„ ì¶”ì¶œ ì„±ê³µ")
            
            # 4. ì„ë² ë”© ì¶”ì¶œ
            user_emb = self.extract_embeddings(user_wave)
            if user_emb is None:
                return {
                    "text": segment.text,
                    "start": segment.start,
                    "end": segment.end,
                    "similarity_score": 0.0,
                    "similarity_feedback": "ë¹„êµ ë¶ˆê°€: ì‚¬ìš©ì ìŒì„± ë¶„ì„ ì‹¤íŒ¨",
                    "pronunciation_quality": "ë¶„ì„ ë¶ˆê°€",
                    "pronunciation_score": 0.0,
                    "pronunciation_feedback": "ì‚¬ìš©ì ìŒì„±ì˜ íŠ¹ì„±ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            ref_emb = self.extract_embeddings(ref_wave)
            if ref_emb is None:
                return {
                    "text": segment.text,
                    "start": segment.start,
                    "end": segment.end,
                    "similarity_score": 0.0,
                    "similarity_feedback": "ë¹„êµ ë¶ˆê°€: ê¸°ì¤€ ìŒì„± ë¶„ì„ ì‹¤íŒ¨",
                    "pronunciation_quality": "ë¶„ì„ ë¶ˆê°€",
                    "pronunciation_score": 0.0,
                    "pronunciation_feedback": "ê¸°ì¤€ ìŒì„±ì˜ íŠ¹ì„±ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # 5. ìœ ì‚¬ë„ ê³„ì‚°
            score = self.calculate_similarity(user_emb, ref_emb)
            feedback = self.get_similarity_feedback(score)
            print(f"ğŸ“Š ìœ ì‚¬ë„: {score:.1f} / 100")
            
            # 6. Parselmouth ë¶„ì„ ì¶”ê°€ (ë³´ì¡° ì§€í‘œ)
            parselmouth_result = self.analyze_pronunciation(user_audio, start_time, end_time)
            
            # 7. ìŒëŸ‰ ë¶„ì„ ì¶”ê°€
            volume_result = self.analyze_volume(user_audio, start_time, end_time)
            
            # ê¸°ë³¸ ë¶„ì„ ê²°ê³¼
            result = {
                "text": segment.text,
                "start": segment.start,
                "end": segment.end,
                "similarity_score": score,
                "similarity_feedback": feedback,
                "pronunciation_quality": parselmouth_result["quality"],
                "pronunciation_score": parselmouth_result["score"],
                "pronunciation_feedback": parselmouth_result["feedback"]
            }
            
            # 1. ë§ì†ë„ ë¶„ì„
            wpm, speed_info = self.analyze_speech_rate(segment)
            if wpm is not None:
                result["speech_rate"] = {
                    "wpm": wpm,
                    "status": speed_info[0],
                    "feedback": speed_info[1]
                }
            
            # 2. ë§ë”ë“¬ê¸° ê°ì§€
            stutter_words, stutter_feedback = self.detect_stuttering(segment.text)
            if stutter_words is not None:
                result["stuttering"] = {
                    "repeated_words": stutter_words,
                    "feedback": stutter_feedback
                }
            
            # 3. ì¹¨ë¬µ ê°ì§€
            silence_duration, silence_feedback = self.analyze_silence(segment, previous_segment)
            if silence_duration is not None:
                result["silence"] = {
                    "duration": silence_duration,
                    "feedback": silence_feedback
                }
            
            # 4. ìŒëŸ‰ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if volume_result is not None:
                result["volume"] = volume_result
                # ìŒëŸ‰ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
                print(f"\nğŸ”Š ìŒëŸ‰ ë¶„ì„: í‰ê·  RMS = {volume_result['rms']:.3f} â†’ {volume_result['emoji']} {volume_result['status']}")
                print(f"ğŸ“¢ ìŒëŸ‰ í”¼ë“œë°±: {volume_result['feedback']}")
                
                # ìŒëŸ‰ ìƒíƒœì— ë”°ë¥¸ ì¶”ê°€ í”¼ë“œë°±
                if volume_result['status'] == "Too Quiet":
                    print("ğŸ’¡ ê°œì„  ì œì•ˆ: ")
                    print("- ë³µì‹í˜¸í¡ì„ í™œìš©í•˜ì—¬ ë°œì„±ì— í˜ì„ ì‹¤ì–´ë³´ì„¸ìš”")
                    print("- ì²­ì¤‘ì„ í–¥í•´ ë” ë˜ë ·í•˜ê²Œ ë°œì„±í•´ë³´ì„¸ìš”")
                elif volume_result['status'] == "Too Loud":
                    print("ğŸ’¡ ê°œì„  ì œì•ˆ: ")
                    print("- í˜¸í¡ì„ ì•ˆì •ì‹œí‚¤ê³  ì°¨ë¶„í•˜ê²Œ ë§í•´ë³´ì„¸ìš”")
                    print("- ë§ˆì´í¬ì™€ì˜ ê±°ë¦¬ë¥¼ ì•½ê°„ ë” ë‘ì–´ë³´ì„¸ìš”")
                else:
                    print("ğŸ’¡ ì˜í•˜ê³  ìˆëŠ” ì : ")
                    print("- ì²­ì¤‘ì´ ë“£ê¸° í¸ì•ˆí•œ ìŒëŸ‰ì„ ì˜ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤")
                    print("- ì´ ìŒëŸ‰ì„ ê³„ì† ìœ ì§€í•´ì£¼ì„¸ìš”")
            
            return result
            
        except Exception as e:
            print(f"\nâŒ êµ¬ê°„ ë¹„êµ ì˜¤ë¥˜: {e}")
            # Fallback: Parselmouth ë¶„ì„ë§Œ ìˆ˜í–‰
            result = self.analyze_pronunciation(user_audio, segment.start, segment.end)
            return {
                "text": segment.text,
                "start": segment.start,
                "end": segment.end,
                "similarity_score": 0.0,
                "similarity_feedback": f"ë¹„êµ ë¶ˆê°€: {str(e)}",
                "pronunciation_quality": result["quality"],
                "pronunciation_score": result["score"],
                "pronunciation_feedback": result["feedback"]
            }

    def transcribe_audio(self, audio_path: str) -> list:
        """ìŒì„± íŒŒì¼ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì „ì‚¬"""
        try:
            print("\nğŸ¯ ìŒì„± ì „ì‚¬ ì¤‘...")
            
            # 1. íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
            if not self.validate_audio_file(audio_path):
                return []
            
            # 2. ì „ì²´ ê¸¸ì´ í™•ì¸
            duration = librosa.get_duration(filename=audio_path)
            print(f"âœ… ì „ì²´ ê¸¸ì´: {duration:.1f}ì´ˆ")
            
            # 3. ì „ì‚¬ ìˆ˜í–‰
            segments, _ = self.model.transcribe(audio_path, language="ko")
            segments = list(segments)
            
            if not segments:
                print("âš ï¸ ê°ì§€ëœ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            print(f"âœ… ì „ì‚¬ ì™„ë£Œ: {len(segments)}ê°œ ë¬¸ì¥ ê°ì§€")
            return segments
            
        except Exception as e:
            print(f"âŒ ì „ì‚¬ ì˜¤ë¥˜: {e}")
            return []

    def generate_summary_statistics(self, results: list) -> dict:
        """ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ìš”ì•½ í†µê³„ ìƒì„±"""
        stats = {
            "total_segments": len(results),
            "poor_pronunciation": {
                "count": 0,
                "segments": []  # (ë¬¸ì¥ ë²ˆí˜¸, ì‹œì‘ ì‹œê°„, ë ì‹œê°„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            },
            "slow_speech": {
                "count": 0,
                "segments": []
            },
            "fast_speech": {
                "count": 0,
                "segments": []
            },
            "silence_detected": {
                "count": 0,
                "segments": []
            },
            "stuttering_detected": {
                "count": 0,
                "segments": []
            },
            "volume_too_quiet": {
                "count": 0,
                "segments": []
            },
            "volume_too_loud": {
                "count": 0,
                "segments": []
            },
            "volume_good": {
                "count": 0,
                "segments": []
            },
            "avg_similarity": 0.0,
            "avg_pronunciation": 0.0,
            "avg_speech_rate": 0.0,
            "avg_rms": 0.0,
            "valid_segments": 0
        }
        
        total_rms = 0.0
        rms_count = 0
        
        for i, result in enumerate(results, 1):
            # ê¸°ë³¸ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¹´ìš´íŠ¸
            if result.get("pronunciation_score", 0) > 0:
                stats["valid_segments"] += 1
                
                # ë°œìŒ í’ˆì§ˆ ë¶„ì„
                if result["pronunciation_score"] < 65:
                    stats["poor_pronunciation"]["count"] += 1
                    stats["poor_pronunciation"]["segments"].append((i, result["start"], result["end"]))
                stats["avg_pronunciation"] += result["pronunciation_score"]
                
                # ìœ ì‚¬ë„ ë¶„ì„
                if result.get("similarity_score", 0) > 0:
                    stats["avg_similarity"] += result["similarity_score"]
                
                # ë§ì†ë„ ë¶„ì„
                if "speech_rate" in result:
                    wpm = result["speech_rate"]["wpm"]
                    stats["avg_speech_rate"] += wpm
                    if wpm < 90:
                        stats["slow_speech"]["count"] += 1
                        stats["slow_speech"]["segments"].append((i, result["start"], result["end"]))
                    elif wpm > 140:
                        stats["fast_speech"]["count"] += 1
                        stats["fast_speech"]["segments"].append((i, result["start"], result["end"]))
                
                # ì¹¨ë¬µ ê°ì§€
                if "silence" in result:
                    stats["silence_detected"]["count"] += 1
                    stats["silence_detected"]["segments"].append((i, result["start"], result["end"]))
                
                # ë§ë”ë“¬ê¸° ê°ì§€
                if "stuttering" in result:
                    stats["stuttering_detected"]["count"] += 1
                    stats["stuttering_detected"]["segments"].append((i, result["start"], result["end"]))
                
                # ìŒëŸ‰ ë¶„ì„
                if "volume" in result:
                    volume = result["volume"]
                    total_rms += volume["rms"]
                    rms_count += 1
                    
                    if volume["status"] == "Too Quiet":
                        stats["volume_too_quiet"]["count"] += 1
                        stats["volume_too_quiet"]["segments"].append((i, result["start"], result["end"]))
                    elif volume["status"] == "Too Loud":
                        stats["volume_too_loud"]["count"] += 1
                        stats["volume_too_loud"]["segments"].append((i, result["start"], result["end"]))
                    else:  # ì ì ˆ
                        stats["volume_good"]["count"] += 1
                        stats["volume_good"]["segments"].append((i, result["start"], result["end"]))
        
        # í‰ê· ê°’ ê³„ì‚°
        if stats["valid_segments"] > 0:
            stats["avg_pronunciation"] /= stats["valid_segments"]
            stats["avg_speech_rate"] /= stats["valid_segments"]
            if stats.get("avg_similarity", 0) > 0:
                stats["avg_similarity"] /= stats["valid_segments"]
        
        # í‰ê·  RMS ê³„ì‚°
        if rms_count > 0:
            stats["avg_rms"] = total_rms / rms_count
        
        return stats

    def generate_overall_feedback(self, stats: dict) -> str:
        """í†µê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© í”¼ë“œë°± ìƒì„±"""
        feedback_parts = []
        
        # 1. ì „ë°˜ì ì¸ ë°œìŒ í‰ê°€
        if stats["avg_pronunciation"] >= 80:
            feedback_parts.append("ì „ë°˜ì ìœ¼ë¡œ ë°œìŒì´ ë§¤ìš° ì•ˆì •ì ì´ê³  ëª…í™•í•©ë‹ˆë‹¤.")
        elif stats["avg_pronunciation"] >= 65:
            feedback_parts.append("ì „ë°˜ì ìœ¼ë¡œ ë°œìŒì´ ì•ˆì •ì ì´ë©° ì´í•´í•˜ê¸° ì¢‹ì€ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
        else:
            feedback_parts.append("ì „ë°˜ì ìœ¼ë¡œ ë°œìŒì˜ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # 2. ë§ì†ë„ í‰ê°€
        if stats["slow_speech"]["count"] > 0 or stats["fast_speech"]["count"] > 0:
            speed_issues = []
            if stats["slow_speech"]["count"] > 0:
                speed_issues.append(f"{stats['slow_speech']['count']}ê°œ ë¬¸ì¥ì—ì„œ ë§ì†ë„ê°€ ëŠë¦¬ê³ ")
            if stats["fast_speech"]["count"] > 0:
                speed_issues.append(f"{stats['fast_speech']['count']}ê°œ ë¬¸ì¥ì—ì„œ ë§ì†ë„ê°€ ë¹ ë¥´ë©°")
            feedback_parts.append(f"ì´ {', '.join(speed_issues)},")
        else:
            feedback_parts.append("ë§ì†ë„ê°€ ì „ë°˜ì ìœ¼ë¡œ ì ì ˆí•˜ë©°,")
        
        # 3. ì¹¨ë¬µê³¼ ë§ë”ë“¬ í‰ê°€
        issues = []
        if stats["silence_detected"]["count"] > 0:
            issues.append(f"{stats['silence_detected']['count']}íšŒì˜ ë¶ˆí•„ìš”í•œ ì¹¨ë¬µ")
        if stats["stuttering_detected"]["count"] > 0:
            issues.append(f"{stats['stuttering_detected']['count']}íšŒì˜ ë§ë”ë“¬")
        
        if issues:
            feedback_parts.append(f"{' ë° '.join(issues)}ì´ ê°ì§€ë˜ì–´ ì „ë‹¬ë ¥ì´ ë‹¤ì†Œ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            feedback_parts.append("ë°œí‘œ íë¦„ì— ë§ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°ì„ ì—°ìŠµí•´ë³´ì„¸ìš”.")
        else:
            feedback_parts.append("ì „ë°˜ì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë°œí‘œ íë¦„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        
        # 4. ìŒëŸ‰ í‰ê°€ ì¶”ê°€
        volume_feedback = []
        if stats["volume_too_quiet"]["count"] > 0:
            volume_feedback.append(f"{stats['volume_too_quiet']['count']}ê°œ ë¬¸ì¥ì—ì„œ ëª©ì†Œë¦¬ê°€ ë„ˆë¬´ ì‘ê³ ")
        if stats["volume_too_loud"]["count"] > 0:
            volume_feedback.append(f"{stats['volume_too_loud']['count']}ê°œ ë¬¸ì¥ì—ì„œ ëª©ì†Œë¦¬ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤")
        
        if volume_feedback:
            feedback_parts.append(f"\nìŒëŸ‰ ë©´ì—ì„œëŠ” {', '.join(volume_feedback)}.")
            if stats["volume_good"]["count"] > stats["total_segments"] * 0.7:  # 70% ì´ìƒì´ ì ì ˆí•œ ê²½ìš°
                feedback_parts.append("í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ë¬¸ì¥ì—ì„œ ìŒëŸ‰ì´ ì ì ˆí•˜ì—¬ ì „ë°˜ì ì¸ ì „ë‹¬ë ¥ì€ ì¢‹ìŠµë‹ˆë‹¤.")
            else:
                feedback_parts.append("ì „ë°˜ì ìœ¼ë¡œ ìŒëŸ‰ ì¡°ì ˆì— ì‹ ê²½ ì“°ë©´ ë” ì¢‹ì€ ë°œí‘œê°€ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
        else:
            feedback_parts.append("\nìŒëŸ‰ì´ ì „ë°˜ì ìœ¼ë¡œ ë§¤ìš° ì ì ˆí•˜ì—¬ ì²­ì¤‘ì´ ë“£ê¸° í¸ì•ˆí•œ ë°œí‘œì…ë‹ˆë‹¤.")
        
        return " ".join(feedback_parts)

    def analyze_volume(self, audio_path: str, start_time: float, end_time: float) -> dict:
        """ìŒëŸ‰(RMS) ë¶„ì„"""
        try:
            # 1. ì˜¤ë””ì˜¤ ë¡œë“œ ë° êµ¬ê°„ ì¶”ì¶œ
            y, sr = librosa.load(audio_path, sr=None)
            if start_time is not None and end_time is not None:
                start_idx = int(start_time * sr)
                end_idx = int(end_time * sr)
                y = y[start_idx:end_idx]

            # 2. RMS ì—ë„ˆì§€ ê³„ì‚° (í”„ë ˆì„ ë‹¨ìœ„)
            frame_length = 2048  # ì•½ 0.1ì´ˆ ë‹¨ìœ„
            hop_length = 512     # í”„ë ˆì„ ê°„ ì´ë™ ê°„ê²©
            rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
            
            # 3. ìœ íš¨í•œ ìŒì„± êµ¬ê°„ì˜ RMSë§Œ ì„ íƒ (ë§¤ìš° ë‚®ì€ ì—ë„ˆì§€ëŠ” ì œì™¸)
            # ì™„ì „í•œ ë¬´ìŒì´ ì•„ë‹Œ, ë§¤ìš° ë‚®ì€ ì—ë„ˆì§€ ì„ê³„ê°’ ì‚¬ìš©
            energy_threshold = 0.005  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ ì„¤ì •
            valid_rms = rms[rms > energy_threshold]
            
            if len(valid_rms) == 0:  # ìœ íš¨í•œ RMSê°€ ì—†ëŠ” ê²½ìš°
                mean_rms = 0.0
            else:
                # ìƒìœ„ 80% êµ¬ê°„ì˜ í‰ê·  RMS ì‚¬ìš© (ë„ˆë¬´ ë‚®ì€ ê°’ ì œì™¸)
                mean_rms = np.percentile(valid_rms, 80)

            # 4. ìŒëŸ‰ ë“±ê¸‰ ë¶„ë¥˜ (ê¸°ì¤€ê°’ ë¯¸ì„¸ ì¡°ì •)
            if mean_rms < 0.015:
                volume_status = "Too Quiet"
                feedback = "ëª©ì†Œë¦¬ê°€ ì‘ìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” í˜ ìˆê²Œ ë§í•´ë³´ì„¸ìš”."
                emoji = "ğŸ“‰"
            elif mean_rms <= 0.05:
                volume_status = "ì ì ˆ"
                feedback = "ëª©ì†Œë¦¬ í¬ê¸°ê°€ ì ì ˆí•˜ì—¬ ë°œí‘œ ì „ë‹¬ë ¥ì´ ì¢‹ìŠµë‹ˆë‹¤."
                emoji = "âœ…"
            else:
                volume_status = "Too Loud"
                feedback = "ëª©ì†Œë¦¬ê°€ ë‹¤ì†Œ ì»¤ì„œ ë¶€ë‹´ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•½ê°„ ë‚®ì¶°ë³´ì„¸ìš”."
                emoji = "ğŸ“ˆ"

            return {
                "rms": mean_rms,
                "status": volume_status,
                "feedback": feedback,
                "emoji": emoji
            }

        except Exception as e:
            print(f"âŒ ìŒëŸ‰ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None

def get_audio_duration(audio_path: str) -> float:
    """ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ì „ì²´ ê¸¸ì´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        if not os.path.exists(audio_path):
            print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {audio_path}")
            return 0.0
            
        y, sr = librosa.load(audio_path)
        duration = librosa.get_duration(y=y, sr=sr)
        
        if duration == 0.0:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ê°€ 0ì´ˆì…ë‹ˆë‹¤: {audio_path}")
            
        return duration
        
    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì˜¤ë¥˜: {e}")
        return 0.0

def get_quality_emoji(quality: str) -> str:
    """í’ˆì§ˆì— ë”°ë¥¸ ì´ëª¨ì§€ ë°˜í™˜"""
    quality_emojis = {
        "ëª…í™•": "ğŸŒŸ",
        "ì–‘í˜¸": "âœ¨",
        "ë³´í†µ": "â­",
        "ë¶ˆëª…í™•": "ğŸ’«",
        "ë¶„ì„ ë¶ˆê°€": "âš ï¸",
        "ë¶„ì„ ì‹¤íŒ¨": "âŒ"
    }
    return quality_emojis.get(quality, "âœ”ï¸")

if __name__ == "__main__":
    import argparse
    
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description="ë°œìŒ í‰ê°€ ì‹œìŠ¤í…œ")
    parser.add_argument("--audio", type=str, help="ë¶„ì„í•  ìŒì„± íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--text", type=str, help="ê¸°ì¤€ í…ìŠ¤íŠ¸")
    parser.add_argument("--text-file", type=str, help="ê¸°ì¤€ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--generate-only", action="store_true", help="ê¸°ì¤€ ìŒì„±ë§Œ ìƒì„±")
    args = parser.parse_args()
    
    try:
        # ê¸°ì¤€ í…ìŠ¤íŠ¸ ë¡œë“œ
        reference_text = None
        if args.text_file:
            try:
                with open(args.text_file, 'r', encoding='utf-8') as f:
                    reference_text = f.read().strip()
            except Exception as e:
                print(f"âŒ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                exit(1)
        elif args.text:
            reference_text = args.text.strip()
        else:
            print("\nğŸ“ ê¸°ì¤€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì…ë ¥ ì™„ë£Œ í›„ ë¹ˆ ì¤„ì—ì„œ Ctrl+Z ë˜ëŠ” Ctrl+D):")
            try:
                lines = []
                while True:
                    try:
                        line = input()
                        lines.append(line)
                    except EOFError:
                        break
                reference_text = '\n'.join(lines).strip()
            except Exception as e:
                print(f"âŒ í…ìŠ¤íŠ¸ ì…ë ¥ ì˜¤ë¥˜: {e}")
                exit(1)
                
        if not reference_text:
            print("âŒ ê¸°ì¤€ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            exit(1)

        print(f"\nğŸ“Š ë¶„ì„ ì‹œì‘")
        print(f"ğŸ“ ê¸°ì¤€ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(reference_text)}ì")

        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = AdvancedSpeechAnalyzer()
        start_process = time.time()
        
        # ê¸°ì¤€ ìŒì„± ìƒì„±
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        ref_path = f"reference_{timestamp}.wav"
        
        # edge-ttsëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ asyncio.run() ì‚¬ìš©
        ref_path, ref_duration = asyncio.run(analyzer.create_reference_audio(reference_text, ref_path))
        if not ref_path:
            print("âŒ ê¸°ì¤€ ìŒì„± ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            exit(1)
        
        print(f"\nğŸ’¾ ê¸°ì¤€ ìŒì„±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {ref_path}")
        print("ğŸ§ ì´ íŒŒì¼ì„ í†µí•´ AIê°€ ìƒì„±í•œ í‘œì¤€ ë°œìŒì„ ë“¤ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # ê¸°ì¤€ ìŒì„±ë§Œ ìƒì„±í•˜ëŠ” ëª¨ë“œì¸ ê²½ìš° ì—¬ê¸°ì„œ ì¢…ë£Œ
        if args.generate_only:
            print("\nâœ… ê¸°ì¤€ ìŒì„± ìƒì„± ì™„ë£Œ!")
            exit(0)

        # ë¶„ì„í•  ìŒì„± íŒŒì¼ ê²½ë¡œ í™•ì¸
        if not args.audio:
            audio_path = input("\në¶„ì„í•  ìŒì„± íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        else:
            audio_path = args.audio
            
        if not os.path.exists(audio_path):
            print(f"âŒ ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
            exit(1)
            
        print(f"ğŸ¤ ì‚¬ìš©ì ìŒì„±: {audio_path}")

        # ìŒì„± ì „ì‚¬ ë° ë¶„ì„
        segments = analyzer.transcribe_audio(audio_path)
        if not segments:
            print("âŒ ìŒì„± ì „ì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            if os.path.exists(ref_path):
                os.remove(ref_path)
            exit(1)

        print("\nğŸ¯ ë°œìŒ í‰ê°€ ê¸°ì¤€:")
        print("ğŸŒŸ ìš°ìˆ˜ (80ì  ì´ìƒ): ë°œìŒì´ ë§¤ìš° ëª…í™•í•˜ê³  ì•ˆì •ì ")
        print("âœ¨ ì–‘í˜¸ (65-79ì ): ë°œìŒì´ ëŒ€ì²´ë¡œ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€")
        print("â­ ë³´í†µ (50-64ì ): ë°œìŒì´ ì´í•´í•  ë§Œí•˜ë‚˜ ê°œì„ ì˜ ì—¬ì§€ ìˆìŒ")
        print("ğŸ’« ë¯¸í¡ (50ì  ë¯¸ë§Œ): ë°œìŒì´ ë¶ˆì•ˆì •í•˜ì—¬ ê°œì„  í•„ìš”")
        print("âš ï¸ ë¶„ì„ ë¶ˆê°€: êµ¬ê°„ ì¶”ì¶œ ë˜ëŠ” ë¹„êµ ì‹¤íŒ¨")
        
        print("\nğŸ” ë¬¸ì¥ë³„ ë°œìŒ ë¶„ì„ ê²°ê³¼:")
        
        # ê° ë¬¸ì¥ë³„ ë¶„ì„ ìˆ˜í–‰
        total_segments = len(segments)
        successful_comparisons = 0
        total_similarity = 0.0
        total_pronunciation = 0.0
        previous_segment = None
        analysis_results = []  # ì „ì²´ ë¶„ì„ ê²°ê³¼ ì €ì¥
        
        for i, segment in enumerate(segments, 1):
            print(f"\nğŸ“ ë¬¸ì¥ {i}/{total_segments} ë¶„ì„ ì¤‘...")
            
            result = analyzer.compare_segment_with_reference(audio_path, ref_path, segment, ref_duration, previous_segment)
            analysis_results.append(result)  # ê²°ê³¼ ì €ì¥
            
            print(f"\nğŸŸ© ë¬¸ì¥ {i}: {result['text']}")
            print(f"â±ï¸ êµ¬ê°„: {result['start']:.1f}ì´ˆ ~ {result['end']:.1f}ì´ˆ")
            
            # AI ìŒì„±ê³¼ì˜ ìœ ì‚¬ë„
            if result['similarity_score'] > 0:
                print(f"ğŸ“Š AI ìŒì„± ìœ ì‚¬ë„: {result['similarity_score']:.1f}/100")
                print(f"ğŸ’¡ ìœ ì‚¬ë„ í”¼ë“œë°±: {result['similarity_feedback']}")
                total_similarity += result['similarity_score']
                successful_comparisons += 1
            else:
                print("ğŸ“Š AI ìŒì„± ë¹„êµ: " + result['similarity_feedback'])
            
            # Parselmouth ë¶„ì„ ê²°ê³¼
            if result['pronunciation_score'] > 0:
                total_pronunciation += result['pronunciation_score']
                
            quality_emoji = get_quality_emoji(result['pronunciation_quality'])
            print(f"{quality_emoji} ë°œìŒ í’ˆì§ˆ: {result['pronunciation_quality']} (ì ìˆ˜: {result['pronunciation_score']:.1f}/100)")
            print(f"ğŸ—£ï¸ ë°œìŒ í”¼ë“œë°±: {result['pronunciation_feedback']}")
            
            # ë§ì†ë„ ì¶œë ¥
            if 'speech_rate' in result:
                print(f"â© ë§ì†ë„: {result['speech_rate']['wpm']:.1f} WPM â†’ {result['speech_rate']['status']}")
                print(f"ğŸ’¬ ì†ë„ í”¼ë“œë°±: {result['speech_rate']['feedback']}")
            
            # ë§ë”ë“¬ê¸° ì¶œë ¥
            if 'stuttering' in result:
                print(f"ğŸ” ë§ë”ë“¬ê¸°: ë‹¨ì–´ '{result['stuttering']['repeated_words']}'ê°€ ë°˜ë³µë¨")
                print(f"ğŸ§  ë§ë”ë“¬ í”¼ë“œë°±: {result['stuttering']['feedback']}")
            
            # ì¹¨ë¬µ ì¶œë ¥
            if 'silence' in result:
                print(f"ğŸ”‡ ì¹¨ë¬µ ê°ì§€: ì´ì „ ë¬¸ì¥ê³¼ì˜ ê°„ê²© {result['silence']['duration']:.1f}ì´ˆ")
                print(f"ğŸ” ì¹¨ë¬µ í”¼ë“œë°±: {result['silence']['feedback']}")
            
            # ìŒëŸ‰ ì¶œë ¥
            if 'volume' in result:
                print(f"\nğŸ”Š ìŒëŸ‰ ë¶„ì„: í‰ê·  RMS = {result['volume']['rms']:.3f} â†’ {result['volume']['emoji']} {result['volume']['status']}")
                print(f"ğŸ“¢ ìŒëŸ‰ í”¼ë“œë°±: {result['volume']['feedback']}")
                
                # ìŒëŸ‰ ìƒíƒœì— ë”°ë¥¸ ì¶”ê°€ í”¼ë“œë°±
                if result['volume']['status'] == "Too Quiet":
                    print("ğŸ’¡ ê°œì„  ì œì•ˆ: ")
                    print("- ë³µì‹í˜¸í¡ì„ í™œìš©í•˜ì—¬ ë°œì„±ì— í˜ì„ ì‹¤ì–´ë³´ì„¸ìš”")
                    print("- ì²­ì¤‘ì„ í–¥í•´ ë” ë˜ë ·í•˜ê²Œ ë°œì„±í•´ë³´ì„¸ìš”")
                elif result['volume']['status'] == "Too Loud":
                    print("ğŸ’¡ ê°œì„  ì œì•ˆ: ")
                    print("- í˜¸í¡ì„ ì•ˆì •ì‹œí‚¤ê³  ì°¨ë¶„í•˜ê²Œ ë§í•´ë³´ì„¸ìš”")
                    print("- ë§ˆì´í¬ì™€ì˜ ê±°ë¦¬ë¥¼ ì•½ê°„ ë” ë‘ì–´ë³´ì„¸ìš”")
                else:
                    print("ğŸ’¡ ì˜í•˜ê³  ìˆëŠ” ì : ")
                    print("- ì²­ì¤‘ì´ ë“£ê¸° í¸ì•ˆí•œ ìŒëŸ‰ì„ ì˜ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤")
                    print("- ì´ ìŒëŸ‰ì„ ê³„ì† ìœ ì§€í•´ì£¼ì„¸ìš”")
            
            print("---------------")
            previous_segment = segment
        
        # ìš”ì•½ í†µê³„ ë° ì¢…í•© í”¼ë“œë°±
        stats = analyzer.generate_summary_statistics(analysis_results)
        
        print("\nğŸ“Š ì‹¤ìˆ˜ ìš”ì•½ í†µê³„:")
        
        # ë°œìŒ ê°œì„  í•„ìš” ë¬¸ì¥
        if stats["poor_pronunciation"]["count"] > 0:
            print(f"\në°œìŒ ê°œì„  í•„ìš” ë¬¸ì¥ ìˆ˜: {stats['poor_pronunciation']['count']}ê°œ")
            segments_info = [f"ë¬¸ì¥ {seg[0]} ({seg[1]:.1f}~{seg[2]:.1f}ì´ˆ)" 
                           for seg in stats["poor_pronunciation"]["segments"]]
            print(f"â†’ {', '.join(segments_info)}")
        
        # ëŠë¦° ë§ì†ë„ ë¬¸ì¥
        if stats["slow_speech"]["count"] > 0:
            print(f"\nëŠë¦° ë§ì†ë„ ë¬¸ì¥ ìˆ˜: {stats['slow_speech']['count']}ê°œ")
            segments_info = [f"ë¬¸ì¥ {seg[0]} ({seg[1]:.1f}~{seg[2]:.1f}ì´ˆ)" 
                           for seg in stats["slow_speech"]["segments"]]
            print(f"â†’ {', '.join(segments_info)}")
        
        # ë¹ ë¥¸ ë§ì†ë„ ë¬¸ì¥
        if stats["fast_speech"]["count"] > 0:
            print(f"\në¹ ë¥¸ ë§ì†ë„ ë¬¸ì¥ ìˆ˜: {stats['fast_speech']['count']}ê°œ")
            segments_info = [f"ë¬¸ì¥ {seg[0]} ({seg[1]:.1f}~{seg[2]:.1f}ì´ˆ)" 
                           for seg in stats["fast_speech"]["segments"]]
            print(f"â†’ {', '.join(segments_info)}")
        
        # ë¶ˆí•„ìš”í•œ ì¹¨ë¬µ ë°œìƒ ë¬¸ì¥
        if stats["silence_detected"]["count"] > 0:
            print(f"\në¶ˆí•„ìš”í•œ ì¹¨ë¬µ ë°œìƒ ë¬¸ì¥ ìˆ˜: {stats['silence_detected']['count']}ê°œ")
            segments_info = [f"ë¬¸ì¥ {seg[0]} ({seg[1]:.1f}~{seg[2]:.1f}ì´ˆ)" 
                           for seg in stats["silence_detected"]["segments"]]
            print(f"â†’ {', '.join(segments_info)}")
        
        # ë§ë”ë“¬ì´ ê°ì§€ëœ ë¬¸ì¥
        if stats["stuttering_detected"]["count"] > 0:
            print(f"\në§ë”ë“¬ì´ ê°ì§€ëœ ë¬¸ì¥ ìˆ˜: {stats['stuttering_detected']['count']}ê°œ")
            segments_info = [f"ë¬¸ì¥ {seg[0]} ({seg[1]:.1f}~{seg[2]:.1f}ì´ˆ)" 
                           for seg in stats["stuttering_detected"]["segments"]]
            print(f"â†’ {', '.join(segments_info)}")
        
        # ìŒëŸ‰ ê´€ë ¨ í†µê³„
        if stats["volume_too_quiet"]["count"] > 0:
            print(f"\nğŸ“‰ ë„ˆë¬´ ì‘ì€ ìŒëŸ‰ ë¬¸ì¥ ìˆ˜: {stats['volume_too_quiet']['count']}ê°œ")
            segments_info = [f"ë¬¸ì¥ {seg[0]} ({seg[1]:.1f}~{seg[2]:.1f}ì´ˆ)" 
                           for seg in stats["volume_too_quiet"]["segments"]]
            print(f"â†’ {', '.join(segments_info)}")
        
        if stats["volume_too_loud"]["count"] > 0:
            print(f"\nğŸ“ˆ ë„ˆë¬´ í° ìŒëŸ‰ ë¬¸ì¥ ìˆ˜: {stats['volume_too_loud']['count']}ê°œ")
            segments_info = [f"ë¬¸ì¥ {seg[0]} ({seg[1]:.1f}~{seg[2]:.1f}ì´ˆ)" 
                           for seg in stats["volume_too_loud"]["segments"]]
            print(f"â†’ {', '.join(segments_info)}")
        
        print(f"\nâœ… ì ì ˆí•œ ìŒëŸ‰ ë¬¸ì¥ ìˆ˜: {stats['volume_good']['count']}ê°œ")
        
        print(f"\nğŸ“ˆ ì „ì²´ í‰ê· :")
        print(f"í‰ê·  ë°œìŒ ì ìˆ˜: {stats['avg_pronunciation']:.1f}/100")
        if stats["avg_similarity"] > 0:
            print(f"í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜: {stats['avg_similarity']:.1f}/100")
        if stats["avg_speech_rate"] > 0:
            print(f"í‰ê·  ë§ì†ë„: {stats['avg_speech_rate']:.1f} WPM")
        print(f"ğŸ“¶ í‰ê·  RMS (ìŒëŸ‰): {stats['avg_rms']:.3f}")
        
        print("\nğŸ“ ì¢…í•© í”¼ë“œë°±:")
        overall_feedback = analyzer.generate_overall_feedback(stats)
        print(overall_feedback)
        
        # ì „ì²´ í†µê³„
        process_time = time.time() - start_process
        print(f"\nâŒ› ì´ ì²˜ë¦¬ ì‹œê°„: {process_time:.1f}ì´ˆ")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if 'ref_path' in locals() and os.path.exists(ref_path):
            os.remove(ref_path) 